"""
Curiosity Engine for Autonomous Exploration

This module implements the curiosity-driven exploration system that generates
exploration goals based on novelty detection and interest models.
"""

import jax
import jax.numpy as jnp
from typing import Dict, List, Tuple, Optional, Any, Union
from dataclasses import dataclass, field
import numpy as np
from enum import Enum
import time

from .novelty_detection import NoveltyDetector, NoveltyScore, PredictionErrorNoveltyDetector


class ExplorationStrategy(Enum):
    """Different exploration strategies."""
    RANDOM = "random"
    NOVELTY_SEEKING = "novelty_seeking"
    UNCERTAINTY_REDUCTION = "uncertainty_reduction"
    COMPETENCE_PROGRESS = "competence_progress"


@dataclass
class ExplorationGoal:
    """Represents an exploration goal generated by curiosity."""
    goal_id: str
    target_observation: jnp.ndarray
    expected_novelty: float
    priority: float
    strategy: ExplorationStrategy
    created_at: float
    attempts: int = 0
    success_rate: float = 0.0
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class InterestRegion:
    """Represents a region of interest in the observation space."""
    center: jnp.ndarray
    radius: float
    interest_level: float
    visit_count: int
    last_visited: float
    novelty_history: List[float] = field(default_factory=list)
    learning_progress: float = 0.0


class InterestModel:
    """
    Models interest in different regions of the observation space.
    
    Tracks learning progress and adapts interest based on competence development.
    """
    
    def __init__(
        self,
        observation_dim: int,
        num_regions: int = 50,
        interest_decay: float = 0.95,
        progress_window: int = 100
    ):
        self.observation_dim = observation_dim
        self.num_regions = num_regions
        self.interest_decay = interest_decay
        self.progress_window = progress_window
        
        # Initialize interest regions
        self.regions: List[InterestRegion] = []
        self._initialize_regions()
        
        # Learning progress tracking
        self.competence_history: List[float] = []
        self.learning_events: List[Tuple[float, jnp.ndarray, float]] = []  # (timestamp, observation, performance)
    
    def _initialize_regions(self) -> None:
        """Initialize interest regions randomly in observation space."""
        key = jax.random.PRNGKey(42)
        
        for i in range(self.num_regions):
            key, subkey = jax.random.split(key)
            center = jax.random.normal(subkey, (self.observation_dim,))
            
            region = InterestRegion(
                center=center,
                radius=1.0,
                interest_level=1.0,
                visit_count=0,
                last_visited=0.0
            )
            self.regions.append(region)
    
    def _find_nearest_region(self, observation: jnp.ndarray) -> Tuple[int, float]:
        """Find the nearest interest region to an observation."""
        distances = []
        for region in self.regions:
            distance = float(jnp.linalg.norm(observation - region.center))
            distances.append(distance)
        
        nearest_idx = int(np.argmin(distances))
        nearest_distance = distances[nearest_idx]
        
        return nearest_idx, nearest_distance
    
    def update_interest(self, observation: jnp.ndarray, novelty_score: float, performance: float = 0.0) -> None:
        """
        Update interest model based on new observation and outcomes.
        
        Args:
            observation: The observed state
            novelty_score: Novelty score for this observation
            performance: Performance/competence measure (optional)
        """
        current_time = time.time()
        
        # Find nearest region
        region_idx, distance = self._find_nearest_region(observation)
        region = self.regions[region_idx]
        
        # Update region statistics
        region.visit_count += 1
        region.last_visited = current_time
        region.novelty_history.append(novelty_score)
        
        # Keep novelty history bounded
        if len(region.novelty_history) > self.progress_window:
            region.novelty_history.pop(0)
        
        # Compute learning progress (reduction in prediction error over time)
        if len(region.novelty_history) >= 2:
            recent_novelty = np.mean(region.novelty_history[-10:])
            older_novelty = np.mean(region.novelty_history[-20:-10]) if len(region.novelty_history) >= 20 else recent_novelty
            region.learning_progress = max(0.0, older_novelty - recent_novelty)
        
        # Update interest level based on learning progress and novelty
        # High interest for regions with moderate novelty and good learning progress
        optimal_novelty = 0.5  # Sweet spot for learning
        novelty_factor = 1.0 - abs(novelty_score - optimal_novelty)
        progress_factor = min(1.0, region.learning_progress * 2.0)
        
        new_interest = 0.3 * novelty_factor + 0.7 * progress_factor
        region.interest_level = 0.8 * region.interest_level + 0.2 * new_interest
        
        # Decay interest in all regions
        for r in self.regions:
            if r is not region:  # Use object identity instead of array comparison
                r.interest_level *= self.interest_decay
        
        # Store learning event
        self.learning_events.append((current_time, observation, performance))
        if len(self.learning_events) > self.progress_window * 2:
            self.learning_events.pop(0)
    
    def get_interest_level(self, observation: jnp.ndarray) -> float:
        """Get interest level for a given observation."""
        region_idx, distance = self._find_nearest_region(observation)
        region = self.regions[region_idx]
        
        # Interest decreases with distance from region center
        distance_factor = jnp.exp(-distance / region.radius)
        return float(region.interest_level * distance_factor)
    
    def get_most_interesting_regions(self, top_k: int = 5) -> List[Tuple[int, InterestRegion]]:
        """Get the most interesting regions."""
        region_scores = [(i, region.interest_level) for i, region in enumerate(self.regions)]
        region_scores.sort(key=lambda x: x[1], reverse=True)
        
        return [(idx, self.regions[idx]) for idx, _ in region_scores[:top_k]]
    
    def adapt_regions(self) -> None:
        """Adapt region centers based on visit patterns."""
        for region in self.regions:
            if region.visit_count > 10:
                # Move region center towards areas of high learning progress
                if len(self.learning_events) > 0:
                    recent_events = [e for e in self.learning_events if e[0] > region.last_visited - 100]
                    if recent_events:
                        # Weighted average of recent observations
                        weights = [e[2] for e in recent_events]  # Use performance as weight
                        if sum(weights) > 0:
                            observations = jnp.array([e[1] for e in recent_events])
                            weights = jnp.array(weights)
                            weights = weights / jnp.sum(weights)
                            
                            new_center = jnp.sum(observations * weights[:, None], axis=0)
                            region.center = 0.9 * region.center + 0.1 * new_center


class CuriosityEngine:
    """
    Main curiosity engine that generates exploration goals based on novelty and interest.
    
    Combines novelty detection with interest modeling to drive autonomous exploration.
    """
    
    def __init__(
        self,
        observation_dim: int,
        novelty_detector: Optional[NoveltyDetector] = None,
        novelty_threshold: float = 0.3,
        exploration_rate: float = 0.1,
        max_goals: int = 10,
        goal_timeout: float = 100.0
    ):
        self.observation_dim = observation_dim
        self.novelty_threshold = novelty_threshold
        self.exploration_rate = exploration_rate
        self.max_goals = max_goals
        self.goal_timeout = goal_timeout
        
        # Initialize components
        if novelty_detector is None:
            self.novelty_detector = PredictionErrorNoveltyDetector(
                input_dim=observation_dim,
                novelty_threshold=novelty_threshold
            )
        else:
            self.novelty_detector = novelty_detector
        
        self.interest_model = InterestModel(observation_dim)
        
        # Goal management
        self.active_goals: List[ExplorationGoal] = []
        self.completed_goals: List[ExplorationGoal] = []
        self.goal_counter = 0
        
        # Exploration statistics
        self.exploration_history: List[Tuple[float, jnp.ndarray, float]] = []
        self.strategy_performance: Dict[ExplorationStrategy, List[float]] = {
            strategy: [] for strategy in ExplorationStrategy
        }
    
    def compute_novelty_score(self, observation: jnp.ndarray) -> NoveltyScore:
        """Compute novelty score for an observation."""
        return self.novelty_detector.compute_novelty(observation)
    
    def update_models(self, observation: jnp.ndarray, next_observation: jnp.ndarray, performance: float = 0.0) -> None:
        """
        Update internal models with new experience.
        
        Args:
            observation: Current observation
            next_observation: Next observation (for prediction learning)
            performance: Performance measure for this experience
        """
        # Update novelty detector
        self.novelty_detector.update_model(observation, next_observation)
        
        # Compute novelty for interest model update
        novelty_score = self.compute_novelty_score(observation)
        
        # Update interest model
        self.interest_model.update_interest(observation, novelty_score.score, performance)
        
        # Store exploration history
        current_time = time.time()
        self.exploration_history.append((current_time, observation, novelty_score.score))
        
        # Maintain history size
        if len(self.exploration_history) > 1000:
            self.exploration_history.pop(0)
    
    def generate_exploration_goals(self, current_observation: jnp.ndarray, num_goals: int = 3) -> List[ExplorationGoal]:
        """
        Generate new exploration goals based on current state.
        
        Args:
            current_observation: Current observation state
            num_goals: Number of goals to generate
            
        Returns:
            List of exploration goals
        """
        goals = []
        current_time = time.time()
        
        # Clean up expired goals
        self._cleanup_expired_goals(current_time)
        
        # Don't generate more goals if we're at capacity
        if len(self.active_goals) >= self.max_goals:
            return goals
        
        # Get most interesting regions
        interesting_regions = self.interest_model.get_most_interesting_regions(top_k=num_goals * 2)
        
        for i, (region_idx, region) in enumerate(interesting_regions[:num_goals]):
            if len(self.active_goals) >= self.max_goals:
                break
            
            # Generate goal near interesting region
            key = jax.random.PRNGKey(int(current_time * 1000) + i)
            
            # Add some randomness around region center
            noise = jax.random.normal(key, (self.observation_dim,)) * region.radius * 0.5
            target_observation = region.center + noise
            
            # Estimate expected novelty
            expected_novelty = self.interest_model.get_interest_level(target_observation)
            
            # Choose exploration strategy based on region characteristics
            if region.visit_count < 5:
                strategy = ExplorationStrategy.NOVELTY_SEEKING
            elif region.learning_progress > 0.1:
                strategy = ExplorationStrategy.COMPETENCE_PROGRESS
            else:
                strategy = ExplorationStrategy.UNCERTAINTY_REDUCTION
            
            # Compute priority based on interest and novelty
            priority = 0.6 * expected_novelty + 0.4 * region.learning_progress
            
            goal = ExplorationGoal(
                goal_id=f"goal_{self.goal_counter}",
                target_observation=target_observation,
                expected_novelty=expected_novelty,
                priority=priority,
                strategy=strategy,
                created_at=current_time,
                metadata={
                    'region_idx': region_idx,
                    'region_visits': region.visit_count,
                    'learning_progress': region.learning_progress
                }
            )
            
            goals.append(goal)
            self.active_goals.append(goal)
            self.goal_counter += 1
        
        # Sort goals by priority
        goals.sort(key=lambda g: g.priority, reverse=True)
        
        return goals
    
    def evaluate_goal_achievement(self, goal: ExplorationGoal, actual_observation: jnp.ndarray, performance: float) -> bool:
        """
        Evaluate whether an exploration goal was successfully achieved.
        
        Args:
            goal: The exploration goal
            actual_observation: The actual observation reached
            performance: Performance measure for this attempt
            
        Returns:
            True if goal was achieved
        """
        # Compute distance to target
        distance = float(jnp.linalg.norm(actual_observation - goal.target_observation))
        
        # Goal is achieved if we're close enough to the target
        achievement_threshold = 1.0  # Adjust based on observation space scale
        achieved = distance < achievement_threshold
        
        # Update goal statistics
        goal.attempts += 1
        if achieved:
            goal.success_rate = (goal.success_rate * (goal.attempts - 1) + 1.0) / goal.attempts
        else:
            goal.success_rate = (goal.success_rate * (goal.attempts - 1)) / goal.attempts
        
        # Update strategy performance
        self.strategy_performance[goal.strategy].append(performance)
        if len(self.strategy_performance[goal.strategy]) > 100:
            self.strategy_performance[goal.strategy].pop(0)
        
        # Move to completed goals if achieved
        if achieved and goal in self.active_goals:
            self.active_goals.remove(goal)
            self.completed_goals.append(goal)
        
        return achieved
    
    def get_current_exploration_target(self) -> Optional[ExplorationGoal]:
        """Get the highest priority active exploration goal."""
        if not self.active_goals:
            return None
        
        # Sort by priority and return highest
        self.active_goals.sort(key=lambda g: g.priority, reverse=True)
        return self.active_goals[0]
    
    def _cleanup_expired_goals(self, current_time: float) -> None:
        """Remove expired goals from active list."""
        expired_goals = []
        for goal in self.active_goals:
            if current_time - goal.created_at > self.goal_timeout:
                expired_goals.append(goal)
        
        for goal in expired_goals:
            self.active_goals.remove(goal)
            # Don't add to completed goals since they expired
    
    def get_exploration_statistics(self) -> Dict[str, Any]:
        """Get comprehensive exploration statistics."""
        stats = {
            'active_goals': len(self.active_goals),
            'completed_goals': len(self.completed_goals),
            'total_goals_generated': self.goal_counter,
            'novelty_detector_stats': self.novelty_detector.get_statistics(),
        }
        
        # Strategy performance
        for strategy, performances in self.strategy_performance.items():
            if performances:
                stats[f'{strategy.value}_mean_performance'] = np.mean(performances)
                stats[f'{strategy.value}_success_count'] = len(performances)
        
        # Interest model statistics
        interesting_regions = self.interest_model.get_most_interesting_regions(top_k=5)
        stats['top_interest_levels'] = [region.interest_level for _, region in interesting_regions]
        stats['total_regions'] = len(self.interest_model.regions)
        
        # Recent exploration activity
        if self.exploration_history:
            recent_novelty = [score for _, _, score in self.exploration_history[-50:]]
            stats['recent_mean_novelty'] = np.mean(recent_novelty)
            stats['recent_novelty_trend'] = np.polyfit(range(len(recent_novelty)), recent_novelty, 1)[0] if len(recent_novelty) > 1 else 0.0
        
        return stats
    
    def reset_exploration_state(self) -> None:
        """Reset exploration state for new episode or environment."""
        self.active_goals.clear()
        self.completed_goals.clear()
        self.goal_counter = 0
        self.exploration_history.clear()
        
        # Reset interest model regions
        self.interest_model._initialize_regions()
        
        # Keep strategy performance history for learning